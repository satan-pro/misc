{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6727/2138725773.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_model.load_state_dict(torch.load('/home/student/Documents/220962244/LAB5/best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without finetuning:\n",
      "\n",
      "Accuracy of the network on the 10000 test images: 31.25%\n",
      "With finetuning of last layer :\n",
      "\n",
      "0 loss: 0.000845152368423527\n",
      "1 loss: 0.0006384846371119973\n",
      "2 loss: 0.0009427841411216427\n",
      "3 loss: 0.0011114842856108252\n",
      "4 loss: 0.00045214682372648324\n",
      "5 loss: 0.0007162620899265509\n",
      "6 loss: 0.000717975437514055\n",
      "7 loss: 0.000819715990949033\n",
      "8 loss: 0.0007440785228062286\n",
      "9 loss: 0.0008654455259156379\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 62.50%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  #\n",
    "])\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "def MNIST_CNN():\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.MaxPool2d(2, 2),\n",
    "        nn.Dropout(0.25),\n",
    "\n",
    "        # Second Convolutional Block\n",
    "        nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.MaxPool2d(2, 2),\n",
    "        nn.Dropout(0.25),\n",
    "\n",
    "        # Flatten layer\n",
    "        nn.Flatten(),\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        nn.Linear(64 * 7 * 7, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(128, 10)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "pretrained_model = MNIST_CNN().to(device)\n",
    "pretrained_model.load_state_dict(torch.load('/home/student/Documents/220962244/LAB5/best_model.pth'))\n",
    "\n",
    "print('Without finetuning:\\n')\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    pretrained_model.eval()\n",
    "    for data in test_loader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "    outputs = pretrained_model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')\n",
    "\n",
    "print('With finetuning of last layer :\\n')\n",
    "\n",
    "num_epochs = 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "last_layer = None\n",
    "for module in reversed(pretrained_model):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        last_layer = module\n",
    "    break\n",
    "\n",
    "if last_layer is None:\n",
    "    raise ValueError(\"No Linear layer found in the model for finetuning.\")\n",
    "\n",
    "optimizer = optim.Adam(last_layer.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = pretrained_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "    print(f'{epoch} loss: {running_loss / len(train_loader)}')\n",
    "    running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    pretrained_model.eval()\n",
    "    for data in test_loader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "    outputs = pretrained_model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/student/.cache/torch/hub/pytorch_vision_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at epoch 0: 0.1876 Acc: 0.0075\n",
      "Valid Acc at epoch 0: 0.9330\n",
      "Train Loss at epoch 1: 0.0516 Acc: 0.0080\n",
      "Valid Acc at epoch 1: 0.9570\n",
      "Train Loss at epoch 2: 0.0226 Acc: 0.0080\n",
      "Valid Acc at epoch 2: 0.9610\n",
      "Train Loss at epoch 3: 0.0155 Acc: 0.0080\n",
      "Valid Acc at epoch 3: 0.9670\n",
      "Train Loss at epoch 4: 0.0272 Acc: 0.0080\n",
      "Valid Acc at epoch 4: 0.9180\n",
      "Accuracy of the network on the 1000 test images: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dir = 'cats_and_dogs_filtered/train'\n",
    "valid_dir = 'cats_and_dogs_filtered/validation'\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "image_datasets = {\n",
    "    'train': datasets.ImageFolder(train_dir, data_transforms['train']),\n",
    "    'valid': datasets.ImageFolder(valid_dir, data_transforms['valid'])\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(image_datasets['train'], batch_size=32, shuffle=True),\n",
    "    'valid': DataLoader(image_datasets['valid'], batch_size=32, shuffle=False)\n",
    "}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'valid']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "model_ft = torch.hub.load('pytorch/vision', 'alexnet', weights='IMAGENET1K_V1')\n",
    "\n",
    "num_ftrs = model_ft.classifier[6].in_features\n",
    "model_ft.classifier[6] = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, dataloaders, dataset_sizes, num_epochs=10, device='cpu'):\n",
    "   for e in range(num_epochs):\n",
    "\n",
    "       model.train()\n",
    "       running_loss = 0.0\n",
    "       running_corrects = 0\n",
    "\n",
    "       for inputs, labels in dataloaders['train']:\n",
    "           inputs = inputs.to(device)\n",
    "           labels = labels.to(device)\n",
    "           optimizer.zero_grad()\n",
    "\n",
    "           outputs = model(inputs)\n",
    "           _, preds = torch.max(outputs, 1)\n",
    "           loss = criterion(outputs, labels)\n",
    "\n",
    "           loss.backward()\n",
    "           optimizer.step()\n",
    "\n",
    "           running_loss += loss.item() * inputs.size(0)\n",
    "       running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "       train_loss = running_loss / dataset_sizes['train']\n",
    "       train_acc = running_corrects.double() / dataset_sizes['train']\n",
    "\n",
    "       print(f'Train Loss at epoch {e}: {train_loss:.4f} Acc: {train_acc:.4f}')\n",
    "\n",
    "       model.eval()\n",
    "       running_corrects = 0\n",
    "\n",
    "       for inputs, labels in dataloaders['valid']:\n",
    "           inputs = inputs.to(device)\n",
    "           labels = labels.to(device)\n",
    "\n",
    "           with torch.no_grad():\n",
    "               outputs = model(inputs)\n",
    "               _, preds = torch.max(outputs, 1)\n",
    "               running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "       val_acc = running_corrects.double() / dataset_sizes['valid']\n",
    "\n",
    "       print(f'Valid Acc at epoch {e}: {val_acc:.4f}')\n",
    "\n",
    "   return model\n",
    "\n",
    "\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, dataloaders, dataset_sizes, num_epochs=5, device=device)\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(dataset_sizes['valid'], 100 * correct / total))\n",
    "\n",
    "\n",
    "evaluate_model(model_ft, dataloaders['valid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6727/1136169680.py:44: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  self.labels = torch.from_numpy(labels)\n",
      "/tmp/ipykernel_6727/1136169680.py:210: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_model.load_state_dict(torch.load('/home/student/Documents/220962244/LAB5/best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best accuracy: 99.13%\n",
      "Epoch 1: Accuracy = 99.13%, Loss = 0.0744\n",
      "Total Parameters: 421834\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 977    1    0    0    0    0    1    1    0    0]\n",
      " [   0 1129    1    2    0    0    0    3    0    0]\n",
      " [   1    0 1024    0    1    0    0    5    1    0]\n",
      " [   0    1    0 1005    0    3    0    0    1    0]\n",
      " [   0    0    0    0  976    0    2    1    1    2]\n",
      " [   2    0    0    5    0  881    1    1    1    1]\n",
      " [   9    2    0    0    1    2  944    0    0    0]\n",
      " [   0    3    1    2    0    0    0 1022    0    0]\n",
      " [   9    0    2    1    0    2    0    2  956    2]\n",
      " [   4    3    0    3    9    4    0    3    1  982]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       980\n",
      "           1       0.99      0.99      0.99      1135\n",
      "           2       1.00      0.99      0.99      1032\n",
      "           3       0.99      1.00      0.99      1010\n",
      "           4       0.99      0.99      0.99       982\n",
      "           5       0.99      0.99      0.99       892\n",
      "           6       1.00      0.99      0.99       958\n",
      "           7       0.98      0.99      0.99      1028\n",
      "           8       0.99      0.98      0.99       974\n",
      "           9       0.99      0.97      0.98      1009\n",
      "\n",
      "    accuracy                           0.99     10000\n",
      "   macro avg       0.99      0.99      0.99     10000\n",
      "weighted avg       0.99      0.99      0.99     10000\n",
      "\n",
      "Model's state_dict:\n",
      "0.weight \t torch.Size([32, 1, 3, 3])\n",
      "0.bias \t torch.Size([32])\n",
      "2.weight \t torch.Size([32])\n",
      "2.bias \t torch.Size([32])\n",
      "2.running_mean \t torch.Size([32])\n",
      "2.running_var \t torch.Size([32])\n",
      "2.num_batches_tracked \t torch.Size([])\n",
      "5.weight \t torch.Size([64, 32, 3, 3])\n",
      "5.bias \t torch.Size([64])\n",
      "7.weight \t torch.Size([64])\n",
      "7.bias \t torch.Size([64])\n",
      "7.running_mean \t torch.Size([64])\n",
      "7.running_var \t torch.Size([64])\n",
      "7.num_batches_tracked \t torch.Size([])\n",
      "11.weight \t torch.Size([128, 3136])\n",
      "11.bias \t torch.Size([128])\n",
      "14.weight \t torch.Size([10, 128])\n",
      "14.bias \t torch.Size([10])\n",
      "Optimizer's state_dict:\n",
      "state \t {0: {'step': tensor(750.), 'exp_avg': tensor([[[[ 5.9944e-04,  8.2004e-05,  1.5737e-04],\n",
      "          [ 1.0406e-04, -1.7766e-04,  3.2166e-04],\n",
      "          [-8.6139e-04, -9.3824e-05,  2.1787e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 2.1176e-03, -8.5826e-04,  9.6984e-03],\n",
      "          [-4.4614e-03,  5.2739e-03,  7.1964e-03],\n",
      "          [-7.1106e-03,  3.1329e-03,  2.5212e-03]]],\n",
      "\n",
      "\n",
      "        [[[-5.0755e-03, -5.0634e-03, -6.3530e-03],\n",
      "          [-5.9213e-03, -5.0511e-03, -2.0074e-03],\n",
      "          [-5.8028e-03, -2.4483e-03,  2.0976e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.9456e-03,  1.6019e-03,  1.2827e-03],\n",
      "          [ 2.1734e-03,  7.2164e-04,  9.5138e-04],\n",
      "          [ 5.3352e-04, -2.5258e-04, -1.5437e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 8.1027e-04,  2.4104e-03,  1.8823e-03],\n",
      "          [ 8.4265e-05,  1.1265e-03, -2.4441e-03],\n",
      "          [ 1.4334e-04, -9.4808e-04, -2.1054e-03]]],\n",
      "\n",
      "\n",
      "        [[[-2.3325e-05, -1.8187e-05, -5.9424e-04],\n",
      "          [-2.4608e-03, -4.0365e-03, -4.1037e-03],\n",
      "          [-1.0815e-03,  1.8430e-03,  2.3964e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1016e-02,  8.9187e-04, -4.5438e-03],\n",
      "          [ 1.0674e-02,  1.0403e-03,  2.7405e-03],\n",
      "          [-4.8515e-03, -4.8732e-03, -8.9241e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0113e-03, -1.9980e-03, -3.0880e-03],\n",
      "          [-2.3141e-03, -5.9850e-03, -1.3242e-03],\n",
      "          [-1.8861e-03, -3.0685e-03,  1.5207e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 3.6180e-04,  1.4529e-04, -7.8733e-04],\n",
      "          [ 8.2012e-04,  2.0148e-04, -8.1587e-04],\n",
      "          [ 4.7626e-04,  4.6994e-04, -4.4791e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 6.4835e-04, -4.2759e-06, -2.2765e-04],\n",
      "          [ 3.1086e-04, -1.1995e-04,  2.6625e-04],\n",
      "          [-4.4751e-04, -1.4885e-04, -1.8391e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0388e-05,  6.0541e-05, -2.2127e-04],\n",
      "          [ 1.4852e-04, -4.2473e-04, -7.5537e-04],\n",
      "          [-4.9434e-04, -5.3092e-04, -3.6435e-05]]],\n",
      "\n",
      "\n",
      "        [[[-6.0401e-03,  3.5799e-04,  9.9997e-04],\n",
      "          [-5.6493e-03, -1.8445e-03,  4.1664e-03],\n",
      "          [-2.9340e-03, -6.8066e-03, -5.8064e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 2.5713e-05,  1.7681e-03,  4.3998e-03],\n",
      "          [ 2.9006e-03,  2.2842e-03,  2.8439e-03],\n",
      "          [ 1.8845e-03,  1.9201e-03,  7.8159e-05]]],\n",
      "\n",
      "\n",
      "        [[[ 3.2543e-03,  5.2472e-03,  2.8319e-03],\n",
      "          [-3.4755e-04, -1.7535e-03,  2.1926e-05],\n",
      "          [-1.8847e-03, -3.4695e-03, -2.2216e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 4.0206e-03,  3.9052e-03,  1.9501e-03],\n",
      "          [ 3.8470e-03, -2.6337e-04,  1.0963e-03],\n",
      "          [-2.6608e-03, -2.7682e-03,  2.2652e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 3.9219e-03,  6.4244e-03,  5.5433e-03],\n",
      "          [ 7.5519e-03,  1.1837e-02,  1.0201e-02],\n",
      "          [ 1.2700e-02,  1.2025e-02,  7.4730e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8162e-03,  8.0710e-04,  1.1845e-03],\n",
      "          [-1.1187e-03, -1.5720e-03,  6.2926e-04],\n",
      "          [-3.8570e-04,  5.2385e-04,  1.6052e-03]]],\n",
      "\n",
      "\n",
      "        [[[-8.8333e-03, -2.1717e-03,  4.5491e-03],\n",
      "          [-2.4970e-03,  1.0878e-02,  1.3623e-02],\n",
      "          [-1.1794e-03,  7.7240e-03,  7.6039e-03]]],\n",
      "\n",
      "\n",
      "        [[[-9.6327e-04, -4.1973e-03, -5.9977e-03],\n",
      "          [-2.4192e-03, -4.8362e-03, -7.5955e-03],\n",
      "          [-3.0490e-03, -4.0643e-03, -3.6049e-03]]],\n",
      "\n",
      "\n",
      "        [[[-3.2171e-03,  2.6994e-05,  2.5061e-04],\n",
      "          [-4.6077e-03, -3.3894e-04, -6.1864e-04],\n",
      "          [-1.4452e-03,  1.6619e-03,  1.1855e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.7384e-03,  4.6267e-04,  3.3292e-03],\n",
      "          [ 8.2070e-04, -2.1213e-04,  2.6520e-03],\n",
      "          [-5.0379e-04,  1.2956e-03,  2.9644e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.0148e-03, -4.9149e-03,  1.1193e-03],\n",
      "          [-3.9310e-03, -5.0196e-03, -2.6003e-06],\n",
      "          [-8.8375e-04, -9.4417e-04,  6.3125e-04]]],\n",
      "\n",
      "\n",
      "        [[[-3.4407e-04,  2.2519e-03,  1.6234e-03],\n",
      "          [-2.5935e-03,  5.6192e-04,  2.5569e-03],\n",
      "          [ 7.5404e-04,  2.4418e-03,  5.5469e-03]]],\n",
      "\n",
      "\n",
      "        [[[-6.0623e-04,  2.7676e-04,  3.9266e-03],\n",
      "          [-1.6653e-03,  1.5740e-03,  7.4532e-03],\n",
      "          [-1.4821e-03,  1.7042e-03, -7.2911e-04]]],\n",
      "\n",
      "\n",
      "        [[[-1.6216e-03,  2.5307e-05, -3.7794e-03],\n",
      "          [-1.0479e-04, -8.9118e-04, -1.1768e-03],\n",
      "          [ 2.8034e-03,  2.8200e-03,  3.1574e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 5.4938e-03,  5.8213e-03,  3.7768e-03],\n",
      "          [ 1.4200e-02,  1.2249e-02,  7.0577e-03],\n",
      "          [ 1.3961e-02,  8.3599e-03,  5.8010e-03]]],\n",
      "\n",
      "\n",
      "        [[[-7.9692e-03, -6.9857e-03, -2.5618e-03],\n",
      "          [-5.2774e-03, -3.5482e-03,  7.7713e-04],\n",
      "          [-3.6152e-03, -1.8373e-03,  3.1209e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 5.9073e-03,  1.8986e-03, -5.8620e-04],\n",
      "          [ 6.4530e-03, -1.4752e-03, -4.4022e-03],\n",
      "          [-1.1835e-03, -3.5377e-03, -6.3808e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 9.6006e-04,  5.4819e-04, -6.6157e-05],\n",
      "          [ 4.8561e-04,  7.8802e-04,  1.1953e-03],\n",
      "          [-1.0529e-03,  7.0075e-05,  1.4511e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 4.3107e-03,  9.9484e-03,  1.0344e-02],\n",
      "          [ 6.3843e-03,  1.5527e-02,  1.0661e-02],\n",
      "          [ 8.8514e-03,  1.4944e-02,  6.7950e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.1417e-02, -3.7609e-03, -4.3764e-03],\n",
      "          [-4.8245e-03, -1.9649e-03, -1.7600e-03],\n",
      "          [-3.2156e-03, -3.5040e-03,  1.6898e-03]]],\n",
      "\n",
      "\n",
      "        [[[-8.8825e-04,  6.8301e-04,  1.0879e-03],\n",
      "          [ 7.8151e-04,  1.7267e-03,  2.9067e-03],\n",
      "          [ 7.5420e-03,  6.6381e-03,  3.6184e-03]]]]), 'exp_avg_sq': tensor([[[[8.9533e-06, 9.8808e-06, 1.2846e-05],\n",
      "          [3.9038e-06, 5.8470e-06, 1.1035e-05],\n",
      "          [4.4206e-06, 6.9767e-06, 1.1430e-05]]],\n",
      "\n",
      "\n",
      "        [[[1.4136e-03, 8.5546e-04, 2.3381e-03],\n",
      "          [2.0320e-03, 1.6518e-03, 2.1814e-03],\n",
      "          [1.7025e-03, 1.1984e-03, 1.7972e-03]]],\n",
      "\n",
      "\n",
      "        [[[3.6030e-04, 4.0628e-04, 4.8218e-04],\n",
      "          [2.9927e-04, 3.2019e-04, 4.4637e-04],\n",
      "          [1.5126e-04, 1.5933e-04, 3.1188e-04]]],\n",
      "\n",
      "\n",
      "        [[[3.5974e-05, 1.6272e-05, 2.9202e-05],\n",
      "          [3.5880e-05, 1.7748e-05, 3.9208e-05],\n",
      "          [4.1838e-05, 4.2065e-05, 5.2729e-05]]],\n",
      "\n",
      "\n",
      "        [[[1.3039e-04, 1.3919e-04, 9.9606e-05],\n",
      "          [1.1264e-04, 1.0617e-04, 9.1672e-05],\n",
      "          [6.1520e-05, 9.9695e-05, 1.2570e-04]]],\n",
      "\n",
      "\n",
      "        [[[7.2553e-04, 3.4531e-04, 5.1567e-04],\n",
      "          [4.3108e-04, 3.7353e-04, 5.8702e-04],\n",
      "          [6.5383e-05, 1.8582e-04, 3.4458e-04]]],\n",
      "\n",
      "\n",
      "        [[[1.3952e-03, 1.4959e-03, 1.3627e-03],\n",
      "          [1.1107e-03, 1.2395e-03, 1.3898e-03],\n",
      "          [9.9748e-04, 1.3993e-03, 1.5985e-03]]],\n",
      "\n",
      "\n",
      "        [[[5.5283e-05, 2.4331e-04, 6.8088e-04],\n",
      "          [5.7753e-05, 3.7796e-04, 5.8640e-04],\n",
      "          [1.3151e-04, 5.3392e-04, 7.1069e-04]]],\n",
      "\n",
      "\n",
      "        [[[8.5972e-06, 4.3328e-06, 6.6748e-06],\n",
      "          [7.9095e-06, 2.1304e-06, 5.6888e-06],\n",
      "          [1.0309e-05, 7.7944e-06, 9.4922e-06]]],\n",
      "\n",
      "\n",
      "        [[[8.7506e-06, 7.9435e-06, 9.0175e-06],\n",
      "          [5.1130e-06, 3.7407e-06, 6.0184e-06],\n",
      "          [3.7785e-06, 1.8024e-06, 4.4712e-06]]],\n",
      "\n",
      "\n",
      "        [[[2.1538e-06, 1.6014e-06, 6.8864e-06],\n",
      "          [7.5717e-07, 5.1758e-06, 1.4468e-05],\n",
      "          [1.4712e-05, 1.5104e-05, 3.3686e-05]]],\n",
      "\n",
      "\n",
      "        [[[1.9093e-03, 1.1382e-03, 8.2007e-04],\n",
      "          [5.5886e-04, 2.4129e-04, 7.6829e-04],\n",
      "          [9.5984e-04, 1.0680e-03, 2.4893e-03]]],\n",
      "\n",
      "\n",
      "        [[[5.6757e-05, 1.4179e-04, 4.3898e-04],\n",
      "          [3.0049e-04, 3.4989e-04, 4.5023e-04],\n",
      "          [3.8637e-04, 4.3621e-04, 4.7507e-04]]],\n",
      "\n",
      "\n",
      "        [[[5.0598e-04, 2.9516e-04, 1.0369e-04],\n",
      "          [2.8628e-04, 1.3206e-04, 4.7378e-05],\n",
      "          [4.9074e-04, 3.0330e-04, 1.2032e-04]]],\n",
      "\n",
      "\n",
      "        [[[4.3811e-04, 4.7305e-04, 2.5337e-04],\n",
      "          [2.8176e-04, 3.5951e-04, 1.3579e-04],\n",
      "          [5.1267e-04, 5.0396e-04, 1.4752e-04]]],\n",
      "\n",
      "\n",
      "        [[[7.3087e-04, 3.5566e-04, 2.1974e-04],\n",
      "          [1.2785e-03, 1.0331e-03, 7.1556e-04],\n",
      "          [1.6921e-03, 1.8919e-03, 1.8315e-03]]],\n",
      "\n",
      "\n",
      "        [[[1.4691e-04, 1.9261e-04, 2.1562e-04],\n",
      "          [9.6549e-05, 1.8009e-04, 2.3947e-04],\n",
      "          [1.3756e-04, 1.8682e-04, 2.7200e-04]]],\n",
      "\n",
      "\n",
      "        [[[2.2607e-03, 2.1508e-03, 1.1830e-03],\n",
      "          [2.1714e-03, 1.9382e-03, 1.9962e-03],\n",
      "          [2.2422e-03, 2.3848e-03, 2.2640e-03]]],\n",
      "\n",
      "\n",
      "        [[[9.6581e-05, 1.4341e-04, 1.2033e-04],\n",
      "          [1.1265e-04, 1.1403e-04, 1.1933e-04],\n",
      "          [9.7416e-05, 5.6678e-05, 4.0386e-05]]],\n",
      "\n",
      "\n",
      "        [[[1.6351e-04, 1.6961e-04, 1.8604e-04],\n",
      "          [1.6770e-04, 1.7684e-04, 1.5162e-04],\n",
      "          [1.5334e-04, 1.3627e-04, 8.0689e-05]]],\n",
      "\n",
      "\n",
      "        [[[1.5395e-04, 2.6560e-04, 3.2673e-04],\n",
      "          [2.7781e-05, 8.5360e-05, 2.5419e-04],\n",
      "          [7.4606e-06, 5.6016e-05, 3.1822e-04]]],\n",
      "\n",
      "\n",
      "        [[[8.4272e-04, 6.0674e-04, 1.9268e-04],\n",
      "          [7.7810e-04, 2.3760e-04, 1.2968e-04],\n",
      "          [6.9961e-04, 1.8583e-04, 1.7973e-04]]],\n",
      "\n",
      "\n",
      "        [[[2.0387e-04, 2.0594e-04, 2.1450e-04],\n",
      "          [2.4504e-04, 1.7730e-04, 1.8326e-04],\n",
      "          [2.1619e-04, 2.5079e-04, 2.7117e-04]]],\n",
      "\n",
      "\n",
      "        [[[6.6756e-05, 6.2791e-05, 3.2526e-04],\n",
      "          [2.0152e-04, 2.8263e-04, 6.2399e-04],\n",
      "          [4.9343e-04, 5.6141e-04, 6.8499e-04]]],\n",
      "\n",
      "\n",
      "        [[[1.1971e-04, 1.2164e-04, 2.2186e-04],\n",
      "          [6.3677e-05, 1.0175e-04, 1.7915e-04],\n",
      "          [7.2783e-05, 1.9030e-04, 2.4332e-04]]],\n",
      "\n",
      "\n",
      "        [[[3.9826e-04, 7.3146e-04, 9.3755e-04],\n",
      "          [6.8742e-04, 8.8825e-04, 8.2201e-04],\n",
      "          [9.3937e-04, 7.3957e-04, 6.8940e-04]]],\n",
      "\n",
      "\n",
      "        [[[5.8281e-04, 4.1430e-04, 4.5516e-04],\n",
      "          [3.4905e-04, 2.3606e-04, 3.5791e-04],\n",
      "          [1.3706e-04, 6.1036e-05, 2.4491e-04]]],\n",
      "\n",
      "\n",
      "        [[[6.9485e-04, 3.9539e-04, 5.0290e-04],\n",
      "          [9.6913e-04, 8.8241e-04, 1.1947e-03],\n",
      "          [9.3568e-04, 1.0344e-03, 1.1166e-03]]],\n",
      "\n",
      "\n",
      "        [[[2.2978e-05, 1.8722e-05, 1.1924e-05],\n",
      "          [2.1046e-05, 1.4343e-05, 8.7732e-06],\n",
      "          [1.9163e-05, 1.2344e-05, 1.2544e-05]]],\n",
      "\n",
      "\n",
      "        [[[5.2947e-04, 1.0655e-03, 1.4136e-03],\n",
      "          [1.2275e-03, 1.1841e-03, 7.1667e-04],\n",
      "          [1.2845e-03, 1.1490e-03, 5.2833e-04]]],\n",
      "\n",
      "\n",
      "        [[[1.1938e-03, 1.4804e-03, 1.3229e-03],\n",
      "          [1.3662e-03, 1.3151e-03, 1.4407e-03],\n",
      "          [1.6074e-03, 1.5773e-03, 1.3152e-03]]],\n",
      "\n",
      "\n",
      "        [[[2.7082e-04, 4.3806e-04, 1.3300e-03],\n",
      "          [5.9833e-04, 1.8370e-04, 5.2917e-04],\n",
      "          [1.4014e-03, 5.6297e-04, 5.4089e-04]]]])}, 1: {'step': tensor(750.), 'exp_avg': tensor([-1.3689e-04,  3.6168e-03,  1.3806e-03,  6.4797e-04,  5.2903e-05,\n",
      "        -1.2408e-03, -6.0713e-05,  9.6008e-04,  1.8669e-05,  2.2420e-04,\n",
      "        -8.6283e-05,  2.2041e-03,  2.9916e-03,  4.0135e-04,  1.0613e-04,\n",
      "        -1.0426e-02,  6.5950e-04,  4.6653e-03,  1.0327e-03,  4.9563e-04,\n",
      "         2.0185e-03,  8.3542e-05,  7.1401e-04,  1.4544e-03,  1.3954e-03,\n",
      "        -9.2710e-04,  3.3256e-03,  2.6164e-05, -4.2053e-04,  2.6062e-04,\n",
      "        -3.0435e-05, -2.4263e-03]), 'exp_avg_sq': tensor([2.1512e-06, 2.4919e-04, 6.6200e-05, 7.2266e-06, 1.4116e-05, 1.3555e-04,\n",
      "        1.9890e-04, 1.6786e-04, 1.7871e-06, 3.2011e-06, 3.0973e-06, 8.3284e-04,\n",
      "        1.4357e-04, 9.4226e-05, 8.1262e-05, 1.0759e-03, 3.3245e-05, 3.3591e-04,\n",
      "        2.9146e-05, 2.7048e-05, 2.4792e-05, 5.9051e-04, 3.7080e-05, 2.1660e-04,\n",
      "        3.4974e-05, 8.0127e-05, 1.1564e-04, 1.8928e-04, 4.1379e-06, 2.8148e-04,\n",
      "        2.2841e-04, 5.8773e-04])}, 2: {'step': tensor(750.), 'exp_avg': tensor([-4.8272e-05, -1.1720e-03, -2.0978e-03, -9.3465e-04, -1.1723e-03,\n",
      "         1.0150e-03,  7.1576e-04, -5.5682e-03,  3.5273e-05,  9.6422e-04,\n",
      "         7.8741e-04,  1.0918e-03, -5.1110e-03,  1.3909e-03,  9.2882e-04,\n",
      "         1.2788e-03,  1.0492e-03,  4.3219e-03, -3.9138e-05, -2.6229e-04,\n",
      "        -1.1367e-03, -1.3003e-03, -6.8167e-05,  3.9262e-04, -1.5025e-03,\n",
      "         2.0687e-03,  1.0587e-03, -1.3285e-03, -1.2993e-03, -1.6569e-04,\n",
      "         1.4503e-03, -3.7068e-04]), 'exp_avg_sq': tensor([3.0228e-05, 5.4896e-05, 5.4464e-05, 3.6535e-05, 3.2581e-05, 6.6526e-05,\n",
      "        5.5107e-05, 6.1643e-05, 3.1860e-05, 1.9795e-05, 2.4194e-05, 1.0440e-04,\n",
      "        4.1183e-05, 7.5583e-05, 7.8548e-05, 1.1550e-04, 5.0377e-05, 1.0227e-04,\n",
      "        3.2334e-05, 3.5165e-05, 2.5922e-05, 7.1556e-05, 4.7529e-05, 5.7894e-05,\n",
      "        4.7259e-05, 5.0522e-05, 4.3368e-05, 6.0936e-05, 2.7308e-05, 5.9858e-05,\n",
      "        7.5406e-05, 7.3610e-05])}, 3: {'step': tensor(750.), 'exp_avg': tensor([ 1.4033e-03,  3.5002e-03,  1.4759e-03,  2.5593e-03, -1.8926e-05,\n",
      "         2.9859e-03,  2.9391e-03, -8.0668e-04,  1.9342e-03,  6.5143e-04,\n",
      "        -2.5930e-03,  1.5832e-03,  6.6527e-04,  1.4097e-03,  3.5134e-03,\n",
      "        -3.0567e-03,  2.4332e-03,  2.7766e-03,  2.1091e-03,  1.6469e-03,\n",
      "        -1.5826e-03, -8.0952e-05,  1.5009e-03,  8.5394e-04,  1.5825e-03,\n",
      "         1.6668e-03,  8.0081e-04,  4.0878e-04,  4.4200e-04, -1.9511e-04,\n",
      "         3.1561e-03, -3.7695e-03]), 'exp_avg_sq': tensor([4.8004e-05, 6.2144e-05, 4.0967e-05, 4.9469e-05, 1.9583e-05, 7.4236e-05,\n",
      "        6.2179e-05, 2.1005e-05, 5.2110e-05, 2.2127e-05, 1.8133e-05, 4.0542e-05,\n",
      "        2.5909e-05, 5.8683e-05, 7.4122e-05, 4.1569e-05, 4.3156e-05, 5.3131e-05,\n",
      "        2.3959e-05, 3.5250e-05, 1.7198e-05, 3.7286e-05, 3.5732e-05, 3.3881e-05,\n",
      "        2.5753e-05, 1.9211e-05, 3.1557e-05, 2.2798e-05, 2.1289e-05, 1.4435e-05,\n",
      "        5.5040e-05, 3.2820e-05])}, 4: {'step': tensor(750.), 'exp_avg': tensor([[[[ 1.9811e-04,  2.6949e-05,  1.3820e-05],\n",
      "          [ 6.3133e-05, -1.3777e-04,  5.6290e-04],\n",
      "          [-2.2527e-04,  1.2561e-04,  4.0898e-04]],\n",
      "\n",
      "         [[ 2.7457e-05,  4.2597e-04,  5.6482e-04],\n",
      "          [-8.7305e-04,  7.7310e-04,  3.6757e-04],\n",
      "          [ 4.2138e-04,  5.4567e-04,  6.7741e-04]],\n",
      "\n",
      "         [[ 9.7942e-04,  4.9252e-04, -1.1737e-04],\n",
      "          [ 3.8437e-04, -8.0504e-04, -2.2507e-04],\n",
      "          [-2.9906e-04, -7.9861e-05,  5.2873e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 9.5049e-04,  5.5205e-04,  6.2992e-04],\n",
      "          [-4.9200e-04, -2.0816e-06, -6.6689e-04],\n",
      "          [ 8.1855e-05,  3.6431e-04,  5.6714e-04]],\n",
      "\n",
      "         [[ 9.2658e-04,  1.1147e-03,  3.3446e-04],\n",
      "          [-1.4750e-03, -4.4853e-04, -4.7057e-05],\n",
      "          [ 1.5873e-04,  6.6541e-04,  4.5135e-04]],\n",
      "\n",
      "         [[-5.2017e-04, -7.5706e-04, -2.9507e-04],\n",
      "          [ 1.1718e-03, -5.8579e-04,  1.8420e-04],\n",
      "          [ 1.4338e-03,  5.2232e-04,  6.5598e-06]]],\n",
      "\n",
      "\n",
      "        [[[-1.4332e-04, -4.8579e-04, -1.2516e-03],\n",
      "          [ 3.4130e-04, -2.7799e-04, -3.4499e-04],\n",
      "          [-4.5043e-04, -1.2532e-04,  6.9764e-04]],\n",
      "\n",
      "         [[ 3.2995e-04, -3.5339e-04,  8.8943e-04],\n",
      "          [ 2.2658e-04, -5.6995e-05,  2.7079e-03],\n",
      "          [-3.4563e-03, -2.6141e-03,  3.0366e-03]],\n",
      "\n",
      "         [[-9.0948e-04, -1.0207e-04, -7.8020e-04],\n",
      "          [-1.3680e-03, -1.2570e-03, -5.5071e-05],\n",
      "          [ 1.7450e-04, -4.6063e-04,  1.1671e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.6420e-04, -8.1526e-05, -9.4491e-04],\n",
      "          [-3.0583e-04,  3.2699e-04,  7.4712e-04],\n",
      "          [-6.5818e-04, -6.1769e-04,  1.5484e-03]],\n",
      "\n",
      "         [[ 9.2421e-04,  6.2757e-04, -4.5442e-06],\n",
      "          [-4.6263e-06,  3.7978e-04,  1.5587e-03],\n",
      "          [-2.5214e-04,  9.1353e-05,  1.5173e-03]],\n",
      "\n",
      "         [[-1.9810e-04,  2.3723e-04, -3.4253e-04],\n",
      "          [-1.5627e-03,  4.1132e-04, -1.7692e-04],\n",
      "          [-2.6495e-04, -1.1226e-03,  8.6939e-05]]],\n",
      "\n",
      "\n",
      "        [[[-1.4344e-04, -5.5602e-04, -8.4164e-04],\n",
      "          [-1.2875e-03, -3.9124e-04, -1.7404e-03],\n",
      "          [-4.8723e-04,  1.4296e-05, -2.8357e-04]],\n",
      "\n",
      "         [[-5.1464e-03, -2.9934e-03, -1.2063e-03],\n",
      "          [-9.3541e-04, -1.6865e-04, -4.3370e-04],\n",
      "          [ 9.8225e-05,  2.0490e-04,  1.0515e-04]],\n",
      "\n",
      "         [[ 6.6593e-04,  5.8733e-04, -6.6015e-04],\n",
      "          [-1.9815e-03, -1.0821e-03, -1.7490e-03],\n",
      "          [-7.9039e-04, -1.3654e-03, -1.4473e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.8583e-05, -1.6530e-03, -2.4727e-04],\n",
      "          [ 8.4306e-04, -9.4357e-04, -4.0663e-04],\n",
      "          [ 6.5240e-04,  3.9302e-04,  1.2814e-04]],\n",
      "\n",
      "         [[-8.4127e-04, -2.0728e-03, -2.2628e-03],\n",
      "          [-1.5597e-03, -1.6957e-03, -2.9060e-04],\n",
      "          [ 6.7363e-04, -8.6217e-04, -6.3583e-05]],\n",
      "\n",
      "         [[ 3.5270e-04,  5.8143e-04,  6.7130e-04],\n",
      "          [ 1.1421e-03,  4.0715e-04, -1.7513e-04],\n",
      "          [ 2.6436e-04,  7.1743e-06, -4.6716e-04]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 8.5584e-04,  5.6141e-04, -4.5453e-04],\n",
      "          [ 6.4689e-04,  2.6532e-04,  1.1394e-03],\n",
      "          [-5.6000e-04, -9.0583e-04, -8.1880e-04]],\n",
      "\n",
      "         [[-4.6118e-04, -4.5911e-04,  8.6145e-04],\n",
      "          [-7.3921e-04,  3.8908e-04,  1.5213e-04],\n",
      "          [-2.4995e-04,  2.3209e-04,  2.1818e-04]],\n",
      "\n",
      "         [[-7.3129e-06,  4.1855e-04,  9.2139e-04],\n",
      "          [ 1.3755e-03,  1.1718e-03, -3.6232e-04],\n",
      "          [-4.1633e-05,  6.6225e-04,  3.1010e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.5907e-04, -4.8189e-04,  6.6344e-04],\n",
      "          [ 5.5324e-04, -4.0683e-04, -1.4060e-03],\n",
      "          [ 1.3140e-04, -3.3108e-04, -1.4785e-04]],\n",
      "\n",
      "         [[ 1.3550e-03, -5.1428e-04,  5.2196e-04],\n",
      "          [ 1.8247e-03,  1.7388e-04,  6.8918e-04],\n",
      "          [ 1.0218e-04, -7.0407e-04,  2.2365e-05]],\n",
      "\n",
      "         [[-5.6507e-04, -4.5185e-04, -5.6666e-04],\n",
      "          [ 2.5371e-04, -2.7829e-04,  2.5606e-04],\n",
      "          [-5.5852e-04,  5.4470e-04,  9.4609e-05]]],\n",
      "\n",
      "\n",
      "        [[[ 6.9164e-04,  7.9261e-04,  9.2363e-04],\n",
      "          [ 2.8703e-04,  4.4828e-04,  3.5144e-04],\n",
      "          [ 1.4923e-04,  2.8293e-04, -1.1304e-05]],\n",
      "\n",
      "         [[ 3.7596e-04,  2.3512e-05, -1.4984e-04],\n",
      "          [ 1.3241e-03,  9.2041e-04, -5.5801e-04],\n",
      "          [-1.8515e-04,  2.2656e-04, -1.1100e-03]],\n",
      "\n",
      "         [[-1.9112e-03, -5.2357e-04, -9.4967e-04],\n",
      "          [-1.3255e-04,  8.7930e-05, -7.3633e-05],\n",
      "          [-7.3665e-04, -4.5173e-04, -5.3691e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.7453e-04, -6.0115e-04, -4.8737e-04],\n",
      "          [-4.3019e-04, -4.6834e-04, -3.3696e-04],\n",
      "          [-1.5465e-04, -6.8303e-04, -6.4634e-04]],\n",
      "\n",
      "         [[ 1.1727e-04, -3.2484e-04, -2.0942e-04],\n",
      "          [ 2.8627e-04,  1.0229e-03, -2.8993e-04],\n",
      "          [-4.9731e-04, -6.4511e-04,  2.5423e-04]],\n",
      "\n",
      "         [[-3.9924e-03, -1.5207e-03, -5.9462e-04],\n",
      "          [-1.3170e-03, -5.7970e-04, -8.1488e-04],\n",
      "          [-1.0057e-03, -1.6605e-04, -8.3328e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 5.7327e-06,  4.2805e-05, -4.4723e-04],\n",
      "          [ 1.6821e-05,  5.2314e-04,  1.3392e-03],\n",
      "          [-5.6093e-05,  1.0390e-03,  7.9057e-04]],\n",
      "\n",
      "         [[ 1.0032e-04,  7.3125e-04,  1.0150e-03],\n",
      "          [ 3.7564e-04,  2.9128e-03,  2.2273e-03],\n",
      "          [ 2.1706e-03,  1.9588e-03,  8.3863e-06]],\n",
      "\n",
      "         [[ 4.5577e-05, -1.9867e-04, -2.7486e-04],\n",
      "          [-7.4873e-05, -6.9373e-04,  7.1596e-04],\n",
      "          [-2.4867e-04,  7.0015e-04,  1.8029e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.2228e-04, -3.3861e-04,  1.0982e-03],\n",
      "          [ 1.6152e-04, -4.4510e-04,  1.6561e-03],\n",
      "          [-4.8332e-04,  2.8777e-03,  6.4567e-04]],\n",
      "\n",
      "         [[-6.8564e-05,  1.5463e-04,  1.0674e-03],\n",
      "          [ 2.4631e-05,  1.0045e-03,  1.0503e-03],\n",
      "          [ 4.0217e-04,  4.4485e-03,  1.2926e-04]],\n",
      "\n",
      "         [[-2.2671e-04, -4.2904e-04,  2.6480e-04],\n",
      "          [-5.3549e-05, -8.7241e-04,  1.1921e-04],\n",
      "          [-5.9108e-05, -1.5458e-03,  4.7343e-05]]]]), 'exp_avg_sq': tensor([[[[4.6353e-06, 3.6378e-06, 4.4799e-06],\n",
      "          [4.1762e-06, 3.9432e-06, 4.7890e-06],\n",
      "          [2.2447e-06, 2.3870e-06, 4.8043e-06]],\n",
      "\n",
      "         [[1.5303e-05, 1.8654e-05, 2.6747e-05],\n",
      "          [1.1491e-05, 1.6453e-05, 9.9214e-06],\n",
      "          [1.1547e-06, 4.0091e-06, 1.0171e-05]],\n",
      "\n",
      "         [[5.7253e-06, 2.7355e-06, 2.7455e-06],\n",
      "          [1.8490e-05, 1.6232e-05, 1.4748e-05],\n",
      "          [1.0760e-05, 1.0365e-05, 9.7171e-06]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.2230e-05, 6.9778e-06, 1.1581e-05],\n",
      "          [8.9559e-06, 1.6044e-05, 1.3997e-05],\n",
      "          [7.1120e-06, 5.7322e-06, 5.6024e-06]],\n",
      "\n",
      "         [[5.7996e-06, 8.7363e-06, 1.6117e-05],\n",
      "          [2.1432e-05, 1.8458e-05, 1.6020e-05],\n",
      "          [7.6037e-06, 4.5475e-06, 9.1781e-06]],\n",
      "\n",
      "         [[6.2949e-06, 6.8283e-06, 7.4237e-06],\n",
      "          [1.0172e-05, 1.0325e-05, 1.2608e-05],\n",
      "          [1.6616e-05, 1.4580e-05, 1.4668e-05]]],\n",
      "\n",
      "\n",
      "        [[[4.5265e-06, 5.5467e-06, 9.0913e-06],\n",
      "          [1.8868e-06, 1.6817e-06, 6.5666e-06],\n",
      "          [3.2027e-06, 6.6715e-06, 1.0497e-05]],\n",
      "\n",
      "         [[6.4182e-06, 1.9563e-06, 8.4710e-06],\n",
      "          [2.5879e-06, 1.5737e-05, 3.5048e-05],\n",
      "          [3.3934e-05, 6.3563e-05, 6.8153e-05]],\n",
      "\n",
      "         [[2.6940e-05, 2.6961e-05, 1.3682e-05],\n",
      "          [1.1864e-05, 8.7826e-06, 1.2410e-05],\n",
      "          [3.3321e-06, 3.4534e-06, 1.6179e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[3.7450e-05, 9.7566e-06, 7.4132e-06],\n",
      "          [2.1709e-05, 7.4764e-06, 8.4382e-06],\n",
      "          [2.8926e-06, 1.1908e-05, 2.8351e-05]],\n",
      "\n",
      "         [[2.9116e-05, 8.6577e-06, 3.2651e-06],\n",
      "          [2.9216e-06, 1.5356e-06, 1.7328e-05],\n",
      "          [3.1420e-06, 3.0627e-05, 5.8778e-05]],\n",
      "\n",
      "         [[5.0949e-05, 1.7251e-05, 6.6667e-06],\n",
      "          [2.6896e-05, 2.1284e-05, 1.3207e-05],\n",
      "          [3.9372e-06, 8.0975e-06, 1.3919e-05]]],\n",
      "\n",
      "\n",
      "        [[[4.9610e-06, 8.2921e-06, 1.1607e-05],\n",
      "          [5.9648e-06, 9.6704e-06, 1.0943e-05],\n",
      "          [6.1364e-06, 8.4407e-06, 8.2300e-06]],\n",
      "\n",
      "         [[3.5579e-05, 4.8481e-05, 3.3720e-05],\n",
      "          [3.1479e-05, 2.0652e-05, 4.7845e-06],\n",
      "          [1.3394e-05, 2.2949e-06, 6.4809e-06]],\n",
      "\n",
      "         [[4.8034e-06, 2.5432e-06, 4.1285e-06],\n",
      "          [1.9550e-05, 1.5810e-05, 2.3516e-05],\n",
      "          [2.0493e-05, 2.7957e-05, 2.0357e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[4.1357e-06, 4.4971e-06, 1.1208e-05],\n",
      "          [4.5642e-06, 4.6900e-06, 3.5570e-06],\n",
      "          [1.1484e-05, 4.2777e-06, 3.9116e-06]],\n",
      "\n",
      "         [[6.0330e-06, 1.1385e-05, 3.2995e-05],\n",
      "          [2.0140e-05, 2.7861e-05, 2.5854e-05],\n",
      "          [2.1885e-05, 1.8566e-05, 9.2805e-06]],\n",
      "\n",
      "         [[5.0872e-06, 3.3404e-06, 3.6361e-06],\n",
      "          [7.7367e-06, 2.9038e-06, 3.9001e-06],\n",
      "          [3.5093e-05, 9.8873e-06, 4.5787e-06]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[9.9135e-06, 9.3861e-06, 8.0050e-06],\n",
      "          [1.0497e-05, 1.1826e-05, 1.0275e-05],\n",
      "          [6.2244e-06, 9.0464e-06, 1.0988e-05]],\n",
      "\n",
      "         [[2.8435e-05, 2.7120e-05, 3.9937e-05],\n",
      "          [1.1933e-05, 2.2610e-05, 3.0531e-05],\n",
      "          [5.9402e-06, 7.0376e-06, 1.7062e-05]],\n",
      "\n",
      "         [[4.3872e-06, 3.3686e-06, 1.2204e-05],\n",
      "          [2.0006e-05, 1.5809e-05, 4.7044e-06],\n",
      "          [2.4708e-05, 2.7609e-05, 3.2641e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[7.0412e-06, 1.4859e-05, 2.1616e-05],\n",
      "          [6.0241e-06, 1.2069e-05, 3.6845e-05],\n",
      "          [1.0987e-05, 1.1271e-05, 1.7639e-05]],\n",
      "\n",
      "         [[9.7077e-06, 1.9256e-05, 3.6273e-05],\n",
      "          [1.7751e-05, 7.7912e-06, 2.5414e-05],\n",
      "          [1.7362e-05, 1.2615e-05, 1.3438e-05]],\n",
      "\n",
      "         [[7.7548e-06, 1.0140e-05, 1.1896e-05],\n",
      "          [1.4930e-05, 7.7913e-06, 1.7473e-05],\n",
      "          [2.9305e-05, 3.6756e-05, 1.4863e-05]]],\n",
      "\n",
      "\n",
      "        [[[8.5391e-06, 1.2318e-05, 1.1717e-05],\n",
      "          [4.4869e-06, 9.8505e-06, 1.2273e-05],\n",
      "          [7.1261e-06, 1.0558e-05, 1.3183e-05]],\n",
      "\n",
      "         [[4.9290e-06, 7.7298e-06, 2.4579e-05],\n",
      "          [1.8107e-05, 2.5071e-05, 1.6636e-05],\n",
      "          [6.9093e-05, 4.9616e-05, 1.5846e-05]],\n",
      "\n",
      "         [[3.0156e-05, 2.5882e-05, 7.3276e-06],\n",
      "          [1.0023e-05, 1.0034e-05, 8.5098e-06],\n",
      "          [5.6140e-06, 1.1972e-05, 1.0943e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[2.0529e-05, 1.4832e-05, 2.8567e-05],\n",
      "          [1.6356e-05, 1.8071e-05, 1.8285e-05],\n",
      "          [2.2129e-05, 3.8920e-05, 1.3126e-05]],\n",
      "\n",
      "         [[2.2692e-05, 5.1189e-06, 1.1726e-05],\n",
      "          [6.4705e-06, 1.4325e-05, 1.3668e-05],\n",
      "          [4.1873e-05, 4.6633e-05, 1.8794e-05]],\n",
      "\n",
      "         [[5.0619e-05, 2.1753e-05, 3.1632e-05],\n",
      "          [3.3539e-05, 1.9802e-05, 1.6462e-05],\n",
      "          [1.7103e-05, 2.2052e-05, 1.5984e-05]]],\n",
      "\n",
      "\n",
      "        [[[3.6024e-07, 1.5463e-06, 6.0818e-06],\n",
      "          [1.5188e-06, 4.6821e-06, 8.2594e-06],\n",
      "          [5.0382e-06, 9.2461e-06, 9.6228e-06]],\n",
      "\n",
      "         [[5.8110e-07, 2.0763e-05, 3.3324e-05],\n",
      "          [2.2152e-05, 3.8805e-05, 4.1863e-05],\n",
      "          [4.3095e-05, 3.7559e-05, 8.0727e-06]],\n",
      "\n",
      "         [[1.5232e-06, 1.6266e-06, 2.6259e-06],\n",
      "          [7.3393e-07, 1.5404e-06, 6.3546e-06],\n",
      "          [4.1247e-06, 7.7419e-06, 1.3782e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.5008e-06, 2.5187e-06, 1.5173e-05],\n",
      "          [2.3402e-06, 1.0369e-05, 2.7357e-05],\n",
      "          [5.7025e-06, 2.9842e-05, 1.2587e-05]],\n",
      "\n",
      "         [[2.5140e-07, 8.4140e-07, 3.4404e-05],\n",
      "          [6.0626e-07, 1.9104e-05, 4.8478e-05],\n",
      "          [1.6072e-05, 5.3163e-05, 3.8330e-05]],\n",
      "\n",
      "         [[3.2468e-06, 6.3758e-06, 3.9350e-06],\n",
      "          [2.0234e-06, 4.0240e-06, 6.9026e-06],\n",
      "          [5.9747e-06, 7.0556e-06, 5.3486e-06]]]])}, 5: {'step': tensor(750.), 'exp_avg': tensor([-2.5577e-04, -1.1714e-03, -3.9524e-04, -4.0251e-04, -2.1351e-05,\n",
      "        -1.4315e-03,  4.4353e-04,  5.0450e-04,  2.3028e-04, -6.8348e-04,\n",
      "         4.7221e-05,  6.7669e-04, -5.2265e-04, -6.0053e-04, -2.0025e-05,\n",
      "         1.1522e-04, -2.9481e-04, -2.3448e-04, -6.6102e-04, -1.4309e-04,\n",
      "         3.8157e-04, -1.4441e-04, -3.6619e-04, -1.3074e-03,  9.0752e-05,\n",
      "        -5.7244e-07,  7.6389e-05,  6.8580e-05,  5.5949e-04,  2.3769e-04,\n",
      "        -6.1821e-04,  4.2986e-04,  9.5727e-05, -5.1652e-04, -1.2819e-04,\n",
      "        -1.5763e-04, -2.7159e-04,  2.6989e-04,  6.6406e-05,  6.4179e-05,\n",
      "        -6.2515e-04, -4.2584e-06,  3.9270e-04,  2.7578e-05,  9.1967e-05,\n",
      "         9.8493e-05, -1.2434e-04, -7.6095e-05, -5.4371e-04,  5.4666e-05,\n",
      "        -5.3141e-04,  1.9428e-04, -6.1755e-05,  9.2538e-04, -3.1907e-04,\n",
      "         2.9911e-04, -3.4039e-04, -4.3144e-05, -1.5355e-04, -5.7618e-05,\n",
      "         7.6950e-05,  4.9874e-05, -7.4761e-04, -2.3278e-04]), 'exp_avg_sq': tensor([1.3605e-06, 4.9294e-06, 9.7665e-07, 2.7900e-06, 9.0687e-07, 4.3399e-06,\n",
      "        1.4849e-06, 2.4912e-06, 6.2432e-07, 4.0982e-06, 9.9410e-07, 3.1422e-06,\n",
      "        6.0748e-06, 2.4267e-06, 3.1726e-06, 3.0035e-06, 1.4285e-06, 1.2907e-06,\n",
      "        2.8045e-06, 1.1626e-06, 6.7865e-06, 1.4265e-07, 3.1817e-06, 5.7373e-06,\n",
      "        8.4280e-07, 5.8729e-07, 1.2282e-06, 2.2127e-06, 1.9657e-06, 1.0756e-06,\n",
      "        5.6300e-06, 1.3679e-06, 4.9989e-07, 5.3674e-06, 1.9101e-06, 2.3622e-06,\n",
      "        7.1810e-06, 4.3468e-07, 9.0326e-07, 4.8573e-07, 2.3896e-06, 2.2072e-06,\n",
      "        2.7049e-06, 3.6520e-06, 1.5031e-06, 1.6451e-06, 1.7944e-06, 3.0408e-06,\n",
      "        6.9627e-07, 3.3889e-07, 1.9917e-06, 7.6160e-07, 1.6972e-07, 2.3058e-06,\n",
      "        3.9996e-06, 8.0094e-07, 1.1442e-06, 9.2381e-07, 3.4257e-06, 2.9085e-06,\n",
      "        3.3254e-06, 1.4261e-06, 8.3154e-06, 1.9060e-06])}, 6: {'step': tensor(750.), 'exp_avg': tensor([ 4.5499e-04, -6.4845e-04, -5.8659e-04, -2.0792e-03,  3.3363e-03,\n",
      "        -3.3598e-04,  4.2621e-03, -1.0686e-04,  3.6183e-03,  1.4391e-03,\n",
      "        -1.4461e-03,  1.8585e-03,  1.3403e-03, -2.7076e-03, -3.9095e-03,\n",
      "        -1.3992e-03,  5.6697e-04,  2.1830e-03, -7.0107e-04,  2.7879e-03,\n",
      "         3.0763e-04,  4.0695e-03, -7.1393e-05, -3.9999e-03,  1.8464e-04,\n",
      "        -3.2361e-04, -3.4961e-03, -1.5075e-03,  2.6284e-03,  7.9709e-04,\n",
      "        -2.2322e-03,  3.8427e-03,  2.1437e-03, -2.3642e-04, -3.0054e-03,\n",
      "         1.1506e-04, -8.4289e-04,  3.3987e-03,  4.4458e-03,  1.4159e-04,\n",
      "         3.8318e-04,  2.4613e-04, -1.4591e-03,  5.7158e-04, -1.5505e-04,\n",
      "         1.1719e-04, -1.0467e-03, -7.0831e-04,  1.0799e-03,  4.1157e-03,\n",
      "         7.6290e-04,  1.2517e-05,  4.3458e-04, -1.7126e-03,  9.4452e-04,\n",
      "         2.8026e-03, -9.1488e-04,  3.8987e-04, -4.5327e-03, -1.4123e-03,\n",
      "        -6.5615e-04, -1.1997e-04, -1.0842e-03, -2.8972e-04]), 'exp_avg_sq': tensor([5.4698e-05, 4.5638e-05, 7.7739e-05, 5.4915e-05, 4.1237e-05, 6.7005e-05,\n",
      "        5.6105e-05, 8.3772e-05, 3.8497e-05, 6.3428e-05, 3.2752e-05, 7.8729e-05,\n",
      "        7.5026e-05, 5.1060e-05, 5.9704e-05, 4.5599e-05, 4.0192e-05, 3.7088e-05,\n",
      "        7.2466e-05, 3.8000e-05, 5.0661e-05, 3.9651e-05, 7.8761e-05, 8.5070e-05,\n",
      "        4.0530e-05, 2.9375e-05, 3.8509e-05, 4.5494e-05, 6.6223e-05, 5.4821e-05,\n",
      "        6.8809e-05, 4.5977e-05, 5.3463e-05, 5.7161e-05, 7.2371e-05, 3.9264e-05,\n",
      "        8.6358e-05, 3.3831e-05, 8.4947e-05, 4.7029e-05, 6.1577e-05, 4.4973e-05,\n",
      "        4.4729e-05, 4.1080e-05, 4.3348e-05, 5.3668e-05, 4.3837e-05, 4.2488e-05,\n",
      "        3.9255e-05, 4.3966e-05, 4.0049e-05, 4.3427e-05, 2.6004e-05, 6.2939e-05,\n",
      "        5.1023e-05, 6.0138e-05, 4.7636e-05, 3.9839e-05, 6.3175e-05, 5.9801e-05,\n",
      "        5.7589e-05, 7.6322e-05, 4.9361e-05, 6.9968e-05])}, 7: {'step': tensor(750.), 'exp_avg': tensor([-2.7429e-03, -1.0065e-03, -2.6022e-03, -1.8968e-03,  5.2358e-04,\n",
      "        -3.2494e-03,  1.9031e-04,  1.7852e-03,  1.1384e-03, -3.3548e-03,\n",
      "         1.2094e-04,  8.0709e-04, -3.4176e-03, -3.9551e-03, -3.0249e-03,\n",
      "        -2.3727e-03, -1.2028e-03,  7.0100e-04, -5.8075e-03, -8.0754e-04,\n",
      "        -4.0642e-03,  2.6474e-03, -1.7117e-03, -4.5827e-03,  9.5351e-05,\n",
      "        -5.6179e-05, -3.0831e-03, -3.6384e-03,  4.5865e-04, -1.7032e-04,\n",
      "        -4.7848e-03,  1.7014e-03, -7.3073e-04, -3.5771e-03, -1.1574e-03,\n",
      "        -2.4252e-03, -3.5069e-03,  6.4632e-04,  9.2498e-04,  7.6734e-04,\n",
      "        -4.1227e-03,  4.2729e-05, -3.3357e-03, -2.5572e-03, -3.6950e-04,\n",
      "        -2.9653e-03, -2.7687e-03,  6.2642e-04,  5.3698e-04,  2.4097e-03,\n",
      "        -7.8687e-04,  6.2439e-04,  6.3521e-04, -3.3609e-03, -3.3182e-03,\n",
      "         6.8447e-04, -1.9212e-03, -1.9927e-03, -3.0093e-03, -5.1497e-04,\n",
      "        -2.4104e-03,  2.4983e-03,  9.1494e-05, -2.2072e-03]), 'exp_avg_sq': tensor([3.0353e-05, 1.5422e-05, 2.3462e-05, 3.5764e-05, 3.8470e-05, 4.9046e-05,\n",
      "        2.1478e-05, 5.2026e-05, 2.9334e-05, 3.9554e-05, 2.1061e-05, 2.3907e-05,\n",
      "        4.0249e-05, 5.4952e-05, 3.0748e-05, 4.0799e-05, 2.0638e-05, 2.0425e-05,\n",
      "        5.9649e-05, 1.8311e-05, 3.8685e-05, 2.5860e-05, 3.5833e-05, 3.9634e-05,\n",
      "        1.7747e-05, 1.8051e-05, 4.3091e-05, 6.9978e-05, 3.5681e-05, 3.3026e-05,\n",
      "        5.8073e-05, 2.8259e-05, 2.6776e-05, 6.0240e-05, 4.9874e-05, 2.9011e-05,\n",
      "        3.8109e-05, 1.7516e-05, 3.5159e-05, 2.4323e-05, 4.5527e-05, 3.6967e-05,\n",
      "        2.9670e-05, 3.5348e-05, 2.1622e-05, 2.7816e-05, 2.5099e-05, 1.5080e-05,\n",
      "        2.2121e-05, 4.4753e-05, 2.0477e-05, 3.1922e-05, 2.1056e-05, 5.2842e-05,\n",
      "        4.2920e-05, 2.5107e-05, 2.7391e-05, 1.9450e-05, 3.4295e-05, 4.7533e-05,\n",
      "        2.8854e-05, 5.4617e-05, 3.6490e-05, 3.1976e-05])}, 8: {'step': tensor(750.), 'exp_avg': tensor([[ 0.0000e+00,  0.0000e+00,  1.4056e-41,  ...,  1.1698e-41,\n",
      "          1.1698e-41,  9.5737e-42],\n",
      "        [ 2.5641e-05,  3.0118e-06, -1.2530e-05,  ...,  3.5226e-06,\n",
      "          1.3679e-05, -7.2887e-05],\n",
      "        [ 8.8530e-07, -7.4412e-06, -4.5385e-06,  ...,  2.4027e-06,\n",
      "          1.1692e-06,  5.2090e-07],\n",
      "        ...,\n",
      "        [ 2.4236e-05,  1.0205e-05,  2.9527e-05,  ...,  1.6295e-05,\n",
      "          1.4392e-06, -1.2469e-04],\n",
      "        [ 2.0780e-05, -2.5162e-06, -7.5253e-05,  ...,  3.2296e-05,\n",
      "          2.5898e-05,  5.8315e-06],\n",
      "        [-1.1340e-05,  2.8969e-05,  1.2007e-05,  ..., -1.4562e-05,\n",
      "         -1.3316e-05,  4.0139e-05]]), 'exp_avg_sq': tensor([[0.0000e+00, 0.0000e+00, 2.3820e-40,  ..., 1.6498e-40, 1.6498e-40,\n",
      "         1.1044e-40],\n",
      "        [1.5251e-08, 1.4392e-08, 2.4507e-08,  ..., 1.0186e-08, 1.0101e-08,\n",
      "         4.2018e-08],\n",
      "        [1.8650e-08, 1.4256e-08, 5.4352e-08,  ..., 1.0885e-08, 9.5481e-09,\n",
      "         7.0089e-08],\n",
      "        ...,\n",
      "        [2.0804e-08, 1.8085e-08, 2.9387e-08,  ..., 7.8193e-09, 1.0039e-08,\n",
      "         3.3183e-08],\n",
      "        [2.9256e-08, 7.0347e-08, 4.4922e-08,  ..., 1.4994e-08, 1.3511e-08,\n",
      "         4.3555e-08],\n",
      "        [6.3074e-09, 8.1677e-09, 1.1921e-08,  ..., 4.1675e-09, 3.8137e-09,\n",
      "         2.7609e-08]])}, 9: {'step': tensor(750.), 'exp_avg': tensor([-8.7143e-41, -1.1053e-04, -1.8943e-05, -6.5693e-05, -1.3315e-11,\n",
      "         0.0000e+00,  1.8177e-04,  5.1223e-04,  5.6377e-05,  2.2621e-04,\n",
      "         2.4204e-05, -8.5635e-05,  0.0000e+00,  0.0000e+00, -1.9878e-04,\n",
      "        -4.3021e-04, -3.2978e-04, -4.7839e-05,  2.0796e-04, -1.1196e-04,\n",
      "         0.0000e+00,  1.2318e-04, -4.8900e-14, -9.5340e-05, -1.7924e-04,\n",
      "         5.2781e-05, -5.2793e-08,  0.0000e+00,  5.1219e-05, -2.3231e-04,\n",
      "         1.4179e-04, -6.7901e-05,  1.7945e-05, -2.7138e-05,  1.8223e-05,\n",
      "         2.1366e-04, -2.4444e-04,  5.6138e-14, -1.0203e-04,  8.3155e-05,\n",
      "        -5.4776e-05,  1.2450e-04,  0.0000e+00,  5.0574e-04, -9.2973e-05,\n",
      "        -7.6136e-05,  1.0319e-04, -7.6722e-05, -1.0308e-13, -1.4145e-04,\n",
      "         0.0000e+00,  5.3220e-05,  1.1329e-04, -5.1619e-05,  2.3815e-04,\n",
      "        -6.4388e-04,  8.5510e-05,  9.2087e-06, -2.0937e-04,  4.2991e-06,\n",
      "         0.0000e+00, -6.4550e-05, -4.2683e-05,  1.1685e-04, -1.8487e-04,\n",
      "         1.1020e-04, -6.8488e-05,  4.5953e-05, -4.4617e-05, -1.5627e-04,\n",
      "         2.4669e-04, -1.7189e-04,  0.0000e+00, -1.2505e-04, -2.2272e-04,\n",
      "         1.6540e-04,  7.2849e-05,  1.8915e-04,  1.8717e-04, -7.8833e-05,\n",
      "        -9.9721e-05,  6.5696e-05,  3.4781e-05,  2.5522e-04, -7.2692e-06,\n",
      "        -3.8218e-06,  1.1352e-05,  1.6766e-04, -9.6410e-05, -8.8405e-10,\n",
      "         0.0000e+00, -3.0192e-04,  1.7834e-04, -9.5520e-05,  0.0000e+00,\n",
      "        -1.5899e-04,  1.6048e-07,  0.0000e+00,  5.2925e-05, -4.5415e-05,\n",
      "         1.6664e-04, -6.5583e-05, -2.8611e-04, -1.1393e-04,  1.4227e-05,\n",
      "         0.0000e+00, -4.0898e-04, -1.5331e-04, -8.8365e-05,  3.5569e-05,\n",
      "        -1.3369e-04,  0.0000e+00,  1.0934e-04,  4.6495e-05, -1.7011e-04,\n",
      "         7.7602e-06,  6.4415e-05,  3.4811e-04,  4.6801e-05,  0.0000e+00,\n",
      "        -1.5248e-04, -2.1279e-04, -8.0307e-06, -2.4944e-05, -5.3870e-05,\n",
      "        -1.2456e-04, -2.0978e-04,  9.0598e-05]), 'exp_avg_sq': tensor([9.1520e-39, 5.3729e-07, 6.4852e-07, 7.4099e-07, 1.1706e-22, 0.0000e+00,\n",
      "        6.6562e-07, 5.6715e-07, 5.1348e-07, 7.1339e-07, 2.7873e-07, 1.8144e-07,\n",
      "        0.0000e+00, 0.0000e+00, 5.2450e-07, 6.0125e-07, 4.9008e-07, 5.0896e-07,\n",
      "        7.7637e-07, 5.5177e-07, 0.0000e+00, 4.7509e-07, 2.8334e-20, 8.2508e-07,\n",
      "        3.9489e-07, 3.2758e-07, 5.8463e-08, 0.0000e+00, 5.4043e-07, 3.2268e-07,\n",
      "        3.3137e-07, 8.2715e-07, 1.8464e-07, 6.4931e-07, 7.9065e-07, 8.5858e-07,\n",
      "        6.5511e-07, 4.0592e-18, 4.8097e-07, 6.4761e-07, 6.3469e-07, 8.7134e-07,\n",
      "        0.0000e+00, 7.2433e-07, 8.3875e-07, 2.0240e-07, 6.6160e-07, 4.4221e-07,\n",
      "        1.9231e-08, 6.0194e-07, 0.0000e+00, 6.8081e-07, 9.1392e-08, 4.3993e-07,\n",
      "        9.0129e-07, 6.3997e-07, 7.2389e-07, 1.0541e-08, 6.1590e-08, 6.2401e-07,\n",
      "        0.0000e+00, 5.4634e-07, 6.1451e-07, 6.9167e-07, 6.8563e-07, 1.8338e-07,\n",
      "        1.1043e-08, 4.8086e-07, 7.9749e-07, 1.0686e-06, 3.0306e-08, 3.8383e-07,\n",
      "        0.0000e+00, 6.5197e-07, 5.9278e-07, 5.6723e-07, 2.6416e-07, 3.0687e-07,\n",
      "        5.4352e-07, 6.9296e-07, 1.1575e-06, 1.4873e-07, 6.6935e-07, 8.9281e-07,\n",
      "        5.5241e-07, 5.1429e-07, 1.2865e-07, 5.4995e-07, 6.5832e-07, 3.2436e-19,\n",
      "        0.0000e+00, 5.3402e-07, 7.7068e-07, 2.9595e-08, 0.0000e+00, 6.5016e-07,\n",
      "        3.5505e-08, 0.0000e+00, 2.9985e-07, 3.5414e-07, 1.6327e-07, 1.9485e-07,\n",
      "        5.1636e-07, 8.5671e-07, 5.8310e-07, 0.0000e+00, 8.2288e-07, 9.7399e-08,\n",
      "        8.3563e-07, 8.1951e-08, 3.5482e-07, 0.0000e+00, 5.6293e-07, 4.6533e-07,\n",
      "        6.5020e-07, 5.7598e-07, 5.4219e-07, 7.8074e-07, 5.8414e-07, 0.0000e+00,\n",
      "        5.3286e-07, 6.3776e-07, 2.1007e-07, 2.3056e-08, 9.4660e-07, 6.0759e-07,\n",
      "        7.1162e-07, 2.7111e-07])}, 10: {'step': tensor(750.), 'exp_avg': tensor([[ 5.6052e-45, -6.6855e-03,  1.1694e-03,  ..., -5.8294e-03,\n",
      "          2.3839e-03,  5.4577e-05],\n",
      "        [ 0.0000e+00,  1.1491e-04,  1.6497e-04,  ...,  5.3138e-05,\n",
      "          1.3207e-03,  2.2948e-03],\n",
      "        [ 5.6052e-45,  1.4409e-02, -9.8756e-06,  ...,  4.0773e-06,\n",
      "         -6.0007e-02, -1.3801e-03],\n",
      "        ...,\n",
      "        [ 5.6052e-45,  3.9308e-04, -1.2473e-05,  ..., -7.2474e-03,\n",
      "          3.9084e-02,  9.8603e-04],\n",
      "        [ 5.6052e-45,  3.5234e-03,  2.4305e-03,  ...,  2.1797e-03,\n",
      "          1.8889e-02,  5.3678e-04],\n",
      "        [ 1.1210e-44,  6.2400e-03,  1.4922e-04,  ..., -1.0222e-03,\n",
      "          8.7350e-05, -3.0589e-03]]), 'exp_avg_sq': tensor([[0.0000e+00, 7.7978e-04, 3.2752e-04,  ..., 2.3123e-04, 1.6254e-03,\n",
      "         3.7095e-05],\n",
      "        [0.0000e+00, 3.0292e-04, 7.9424e-04,  ..., 4.9522e-04, 3.7915e-04,\n",
      "         1.2734e-04],\n",
      "        [0.0000e+00, 4.0807e-04, 1.0779e-04,  ..., 1.9663e-04, 7.3191e-03,\n",
      "         1.0440e-05],\n",
      "        ...,\n",
      "        [0.0000e+00, 2.5340e-04, 1.8706e-05,  ..., 4.9184e-03, 5.5085e-03,\n",
      "         9.3977e-04],\n",
      "        [0.0000e+00, 2.3059e-04, 4.9097e-03,  ..., 4.4647e-04, 5.1120e-04,\n",
      "         2.4021e-04],\n",
      "        [0.0000e+00, 5.0196e-03, 9.7702e-05,  ..., 1.2702e-02, 2.9785e-04,\n",
      "         1.9691e-03]])}, 11: {'step': tensor(750.), 'exp_avg': tensor([-5.1926e-04, -3.2896e-03, -6.5189e-04, -5.2984e-04,  2.3509e-03,\n",
      "        -3.7451e-04,  1.7180e-03, -1.1459e-04,  1.3674e-03,  4.3340e-05]), 'exp_avg_sq': tensor([2.2067e-05, 1.7107e-05, 3.2376e-05, 2.6519e-05, 2.8361e-05, 2.4811e-05,\n",
      "        1.8454e-05, 3.4099e-05, 3.0898e-05, 4.4140e-05])}}\n",
      "param_groups \t [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn import metrics\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "T = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "\n",
    "class CustomMNISTDataset(Dataset):\n",
    "    def __init__(self, folder_path, train=True, transform=None):\n",
    "        self.folder_path = folder_path\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load images and labels\n",
    "        if train:\n",
    "            images_file = r'/home/student/Documents/220962244/LAB4/MNIST/raw/train-images-idx3-ubyte'\n",
    "            labels_file = r'/home/student/Documents/220962244/LAB4/MNIST/raw/train-labels-idx1-ubyte'\n",
    "        else:\n",
    "            images_file = r'/home/student/Documents/220962244/LAB4/MNIST/raw/t10k-images-idx3-ubyte'\n",
    "            labels_file = r'/home/student/Documents/220962244/LAB4/MNIST/raw/t10k-labels-idx1-ubyte'\n",
    "\n",
    "        # Read binary files\n",
    "        with open(images_file, 'rb') as f:\n",
    "            f.read(16)\n",
    "            images = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "            images = images.reshape(-1, 28, 28)\n",
    "\n",
    "        with open(labels_file, 'rb') as f:\n",
    "            f.read(8)\n",
    "            labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "\n",
    "        # Create a copy of the array to make it writable\n",
    "        self.images = torch.from_numpy(images.copy()).float() / 255.0\n",
    "        self.labels = torch.from_numpy(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx].numpy()\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Reshape to 2D if needed\n",
    "        if image.ndim == 1:\n",
    "            image = image.reshape(28, 28)\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # # Ensure image is 2D with channel dimension\n",
    "        # image = image.unsqueeze(0)  # Add channel dimension for CNN\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomMNISTDataset('MNIST', train=True, transform=T)\n",
    "test_dataset = CustomMNISTDataset('MNIST', train=False, transform=T)\n",
    "\n",
    "# Split validation set\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_data, val_data = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_dl = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_dl = DataLoader(val_data, batch_size=64)\n",
    "test_dl = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "\n",
    "def MNIST_CNN():\n",
    "    model = nn.Sequential(\n",
    "        # First Convolutional Block\n",
    "        nn.Conv2d(1, 32, kernel_size=3, padding=1),  # Input: 1 channel, Output: 32 channels\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.MaxPool2d(2, 2),  # Reduce spatial dimensions\n",
    "        nn.Dropout(0.25),\n",
    "\n",
    "        # Second Convolutional Block\n",
    "        nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.MaxPool2d(2, 2),\n",
    "        nn.Dropout(0.25),\n",
    "\n",
    "        # Flatten layer\n",
    "        nn.Flatten(),\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        nn.Linear(64 * 7 * 7, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(128, 10)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def validate(model, data):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data:\n",
    "            images = images.to('cpu')\n",
    "            labels = labels.to('cpu')\n",
    "            x = model(images)\n",
    "            _, pred = torch.max(x, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (pred == labels).sum().item()\n",
    "    return correct / total * 100\n",
    "\n",
    "\n",
    "def train(pretrained_model,epochs):\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    max_accuracy = 0\n",
    "    best_model = None\n",
    "\n",
    "    # Use CNN model\n",
    "    cnn = pretrained_model\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(cnn.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        cnn.train()\n",
    "        epoch_loss = 0\n",
    "        for images, labels in train_dl:\n",
    "            # No need to flatten images for CNN\n",
    "            images = images.to('cpu')\n",
    "            labels = labels.to('cpu')\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = cnn(images)\n",
    "            loss = loss_fn(pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Validate after each epoch\n",
    "        accuracy = validate(cnn, val_dl)\n",
    "        accuracies.append(accuracy)\n",
    "        losses.append(epoch_loss / len(train_dl))\n",
    "\n",
    "        if accuracy > max_accuracy:\n",
    "            best_model = copy.deepcopy(cnn)\n",
    "            max_accuracy = accuracy\n",
    "            print(f\"New best accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Accuracy = {accuracy:.2f}%, Loss = {epoch_loss / len(train_dl):.4f}\")\n",
    "\n",
    "        def count_parameters(model):\n",
    "            return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "        # Usage\n",
    "        total_params = count_parameters(cnn)\n",
    "        print(f\"Total Parameters: {total_params}\")\n",
    "        torch.save(best_model.state_dict(),'./best_model.pth')\n",
    "    # Plot accuracy and loss curves\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(accuracies)\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(losses)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return best_model,optimizer\n",
    "\n",
    "\n",
    "def predict(model, data):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data:\n",
    "            images = images.to('cpu')\n",
    "            labels = labels.to('cpu')\n",
    "            x = model(images)\n",
    "            _, pred = torch.max(x, 1)\n",
    "            y_pred.extend(pred.numpy())\n",
    "            y_true.extend(labels.numpy())\n",
    "    return np.array(y_pred), np.array(y_true)\n",
    "\n",
    "\n",
    "pretrained_model = MNIST_CNN()\n",
    "pretrained_model.load_state_dict(torch.load('/home/student/Documents/220962244/LAB5/best_model.pth'))\n",
    "\n",
    "# Train the model\n",
    "model,optimizer = train(pretrained_model,1)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred, y_true = predict(model, test_dl)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = metrics.confusion_matrix(y_true, y_pred, labels=np.arange(0, 10))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(metrics.classification_report(y_true, y_pred))\n",
    "\n",
    "\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), \"model_state_dict.pth\")\n",
    "# Print and save optimizer's state dictionary\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
    "\n",
    "\n",
    "torch.save(optimizer.state_dict(), \"optimizer_state_dict.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
